# 减少锁的竞争
#Java #Multithreading 

串行操作会降低可伸缩性，并且上下文的切换也会降低性能。而锁上发生频繁出现竞争，也会导致这两种问题的出现，因此减少锁的竞争能够提高性能和可伸缩性。

在对独占锁保护的资源进行访问时，将采用串行方式以避免数据遭到破坏。而获得这种安全性所付出的代价是限制代码的可伸缩性，尤其是锁上发生持续竞争时。

> 并发程序中，影响可伸缩性的最主要威胁就是独占方式的资源锁。

锁发生竞争的可能性有两个影响因素：锁的请求频率、每次持有锁的时间[^1]。如果两者的乘积很小，则大多数获取锁的操作都不会发生竞争，因此在该锁上的竞争不会对可伸缩性造成严重的影。然而，如果在锁上的请求量很高，那么需要获取该锁的线程将被阻塞并等待。在极端情况下，即使仍有大量工作等待完成，处理器也会被闲置。

[^1]: 这是 Little 定律的必然结论，也是排队理论的一个推论，“在一个稳定的系统中，顾客的平均数量等于他们的平均到达率乘以在系统中的平均停留时间”。

有 3 种方式可以降低锁的竞争程度：

+ 减少锁的持有时间。
+ 降低锁的请求频率。
+ 使用带有协调机制的独占锁，这些锁允许更高的并发性。

## 缩小锁的范围

降低锁发生竞争的有效方法之一就是尽可能的缩短锁的持有时间。例如：将一些无关紧要的代码移出同步代码快，尤其是开销大或阻塞的操作。

例如，将一个锁不必要地持有过长的时间：

```java
public class AttributeStore {

    private final Map<String, String> attributes = new HashMap<>();

    public synchronized boolean userLocationMatches(String name, String regexp) {
        String key = "users." + name + ".location";
        String location = attributes.get(key);
        if (location == null) {
            return false;
        } else {
            return Pattern.matches(regexp, location);
        }
    }
}
```

AttributeStore 类中我们将整个方法逻辑都用锁保护着，而实际我们只需要在访问共享变量 attributes 时才需要加锁保护。

可以通过缩小 userLocationMatches 方法中锁的作用范围，能极大地减少在持有锁时需要执行指令的数量。根据 Amdahl 定律，这样消除了限制可伸缩性的一个因素，因为串行代码的总量减少了。

例如，减少锁的持有时间：

```java
public class BetterAttributeStore {

    private final Map<String, String> attributes = new HashMap<>();

    public boolean userLocationMatches(String name, String regexp) {
        String key = "user." + name + ".location";
        String location;
        synchronized (this) {
            location = attributes.get(key);
        }
        if (location == null) {
            return false;
        } else {
            return Pattern.matches(regexp, location);
        }
    }
}
```

尽管缩小同步代码块能提高可伸缩性，但同步代码块不能太小。一些需要采用原子方式的操作必须包含在一个同步块中。除此之外，在保证正确性的前提下，将一个同步代码块分解为多个同步代码块时，反而会对性能提升产生负面影响。[^2]

[^2]: 如果 JVM 执行锁粒度粗化操作，可能分解的多个同步块又重新合并起来。

在实际情况下，仅当可以将一些“大量”的计算或阻塞操作从同步代码块中移出时，才应该考虑同步代码块的大小。

## 减少锁的竞争

另一种减少锁的持有时间的方式就是降低线程请求锁的频率，从而减少发生竞争的可能。实现这一目的可以通过锁分解和锁分段等技术实现，这些技术中将采用多个相互独立的锁来保护变量，减少锁操作的粒度，并能实现更高的可伸缩性。然而，使用锁越多就越有发生死锁的风险。

如果一个锁需要保护多个相互独立的状态变量，那么可以将这个锁分解为多个锁，并且每个锁只保护一个变量，从而提高可伸缩性，并最终降低每个锁被请求的频率。

例如，对锁进行分解：

```java
public class ServerStatus {

    public final Set<String> users;

    public final Set<String> queries;

    public ServerStatus(Set<String> users, Set<String> queries) {
        this.users = users;
        this.queries = queries;
    }

    public synchronized void addUser(String u) {
        users.add(u);
    }

    public synchronized void addQuery(String q) {
        queries.add(q);
    }

    public synchronized void removeUser(String u) {
        users.remove(u);
    }

    public synchronized void removeQuery(String q) {
        queries.remove(q);
    }
}
```

ServerStatus 类中 users 和 queries 变量之间相互独立，可以通过对锁分解，每个新的细粒度的锁上的访问量将比最初的访问量少：

```java
public class ServerStatusAfterSplit {

    public final Set<String> users;

    public final Set<String> queries;

    public ServerStatusAfterSplit(Set<String> users, Set<String> queries) {
        this.users = users;
        this.queries = queries;
    }

    public void addUser(String u) {
        synchronized (users) {
            users.add(u);
        }
    }

    public void addQuery(String q) {
        synchronized (queries) {
            queries.add(q);
        }
    }

    public void removeUser(String u) {
        synchronized (users) {
            users.remove(u);
        }
    }

    public void removeQuery(String q) {
        synchronized (queries) {
            queries.remove(q);
        }
    }
}
```

如果锁上存在适中而不是激烈的竞争，通过将一个锁分解为两个锁，可以最大限度地提高性能。如果对竞争适中的锁进行分解时，实际上是把这些锁转变为非竞争的锁，从而有效地提高性能和可伸缩性。对竞争并不激烈的锁进行分解，那么在性能和吞吐量等方面带来的提升将非常有限，但是也会提高性能随着竞争提高而下降的拐点值。

## 锁分段

把一个竞争激烈的锁分解为两个锁时，这两个锁可能都存在激烈的竞争。虽然采用两个线程并发执行能提高一部分可伸缩性，但在一个拥有多个处理器的系统中，仍然无法给可伸缩性带来极大的提高。

某些情况下，可以将锁分解技术进一步扩展为对一组独立对象上的锁进行分解，这种情况被称为**锁分段**。在锁分段的技术下，有大量处理器的系统在高访问量的情况下实现更高的并发性，还可以进一步增加锁的数量，但当你能证明并发写入线程的竞争足够激烈并需要突破这个限制时，才能将锁分段的数量进行增加。

锁分段存在一个劣势：与采用单个锁来实现独占访问相比，要获得多个锁来实现独占访问将更加困难并且开销更高。通常，在执行一个操作时最多只需获取一个锁，但在某些需要加锁整个容器的情况下，就需要获取分段锁中的所有的锁。

例如，在基于散列的 Map 中使用锁分段技术：

```java
public class StripedMap {

    private static final int N_LOCKS = 16;

    private final Node[] buckets;
    
    private final Object[] locks;

    private static class Node {
        Node next;
        Object key;
        Object value;
    }

    public StripedMap(int numBuckets) {
        buckets = new Node[numBuckets];
        locks = new Object[N_LOCKS];
        for (int i = 0; i < N_LOCKS; i++) {
            locks[i] = new Object();
        }
    }

    private int hash(Object key) {
        return Math.abs(key.hashCode() % buckets.length);
    }

    public Object get(Object key) {
        int hash = hash(key);
        synchronized (locks[hash % N_LOCKS]) {
            for (Node m = buckets[hash]; m != null; m = m.next) {
                if (m.key.equals(key)) {
                    return m.key;
                }
            }
        }
        return null;
    }
    
    public void clear() {
        for (int i = 0; i < buckets.length; i++) {
            synchronized (locks[i % N_LOCKS]) {
                buckets[i] = null;
            }
        }
    }
}
```

## 避免热点域

锁分解和锁分段可以提高可伸缩性，因为它们可以使不同的线程在不同的数据（或者同一数据的不同部分）上操作，而且不会相互干扰。如果程序采用锁分段技术，那么一定要表现出在锁上的竞争频率高于在锁保护的数据上发生竞争的频率。如果一个锁保护两个独立变量 X 和 Y，并且线程 A 想要访问 X，而线程 B 想要访问 Y，那么这两个线程不会在任何数据上发生竞争，即使它们会在同一个锁上发生竞争。

当每个操作都请求多个变量时，锁的粒度将很难降低。这是在性能和可伸缩性之间相互制衡的另一方面，一些常见的优化措施，例如将一些反复计算的结果缓存起来，都会引入一些“热点域”，而这些热点域往往会限制可伸缩性。

例如 HashMap 的 size 方法，一种常见的优化措施是，在插入和删除元素时更新一个计数器，虽然这在 put 和 remove 等方法中略微增加了一些开销，以确保计数器是最新的值，但这将把 size 方法的开销从 $O(n)$ 降低到 $O(1)$。

在单线程或者采用完全同步的实现中，使用一个独立的计数能很好地提高类似 size 和 isEmpty 这些方法的执行速度，但却导致更难以提升实现的可伸缩性。因为每次变更这个共享的计数器时，也会重新导致在使用独占锁时存在的可伸缩性问题。为了避免这个问题，ConcurrentHashMap 将 size 设置为对每个分段进行枚举并将每个分段中的元素数量相加，而不是维护一个全局计数。

## 一些代替独占锁的方法

第三种降低竞争锁的影响的技术就是放弃使用独占锁，从而有助于使用一种友好并发的方式来管理共享状态。例如，使用并发容器、读 - 写锁、不可变对象以及原子变量。

ReadWriteLock 可以在多个读操作都不会修改共享资源时，那么这些读取操作可以同时访问该共享资源，但在执行写入操作时必须以独占方式来获取锁。对于读取操作占多数的数据结构，可以提供比独占锁更高的并发性。而对于只读的数据结构，其中包含的不变性可完全不必加锁。

原子变量提供了在整数或者对象引用上的细粒度原子操作，并且使用了现代处理器中提供的底层并发原语 CAS。因此它提供了一种降低更新“热点域”开销的方式，但需要保证这些域不会与其他变量参与到不变性条件中。

## 监测 CPU 的利用率

当测试可伸缩性时，通常要确保处理器得到充分的利用。一些工具，例如 UNIX 系统上的 vmstat 和 mpstat，或者 Windows 系统的 perfmon，都能给出处理器的“忙碌”状态。

如果所有的 CPU 的利用率并不均匀（有些 CPU 忙碌，有些 CPU 闲置），则首要工作是进一步找出程序中的并行性。不均匀的利用率表示大多数计算都是由一小组线程完成的，并且应用程序没有利用其他的处理器。

如果 CPU 利用率不足，通常有以下几种原因。

**负载不足**。测试程序可能没有足够多的负载，可以在测试时增加负载，并检查利用率、响应时间和服务时间等指标的变化。如果产生足够多的负载可以将应用程序达到饱和，则需要需要大量的计算机能耗，并且问题可能在于客户端是否具有足够的能力提供足够多的负载。

**I/O 密集**。可以通过 iostat 或 perfmon 来判断某个应用程序是否是磁盘 I/O 密集型，或者通过监测网络流量来判断是否需要高带宽。

**外部限制**。如果应用程序依赖于外部服务，则性能瓶颈可能并不在自己的代码中。可以借助分析工具判断等待外部服务结果的响应时间。

**锁竞争**。使用分析工具可以知道程序中存在何种程度的锁竞争，以及在哪些锁上存在“激烈的竞争”。可通过随机取样触发一些线程转储并在其中查找在锁上发生竞争的线程。如果线程由于等待某个锁而被阻塞，那么栈帧中可以知道它所等待的锁。而对于竞争激烈的锁，通常至少会有一个线程在等待获取它，因此将在线程转储中频繁出现。

如果应用程序正在使 CPU 保持忙碌状态，那么可以使用监听工具来判断是否可以通过增加额外的 CPU 来提升程序的性能。如果 CPU 的利用率很高，并且总会有可运行的线程在等待 CPU，那么当增加更多的处理器时，程序的性能可能会得到提升。

## 向对象池说“不”

在 JVM 早期版本中，对象分配和垃圾回收等操作的执行速度非常慢，但在后续的版本中，这些操作的性能得到了极大提高。事实上，现在 Java 的分配操作已经比 C 语言的 malloc 调用更快：在 HotSpot 1.4.x 和 5.0 中，“new Object”的代码大约只包含 10 条机器指令。

为了解决“缓慢的”对象生命周期问题，许多开发人员都选择使用对象池技术。但在并发程序中，由于多个线程访问对象池中请求一个对象，因此对象池需要通过某种同步来协调对象数据的访问，从而可能使某个线程被阻塞。

如果某个线程由于锁竞争被阻塞，那么这种阻塞的开销将是内存分配操作开销的数百倍，因此即使对象池带来的竞争很小，也可能形成一个可伸缩性瓶颈。

> 通常，对象分配操作的开销比同步的开销更低。



